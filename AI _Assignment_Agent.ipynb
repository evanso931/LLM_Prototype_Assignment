{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Assignment Agent\n",
    "\n",
    "This script is designed to evaluate the performance of a generative AI model in answering questions accurately based on provided data. It utilizes the Google Gemini And OpenAI ChatGPT API's to generate answers to a set of questions derived from a JSON file containing training data. The script processes each question, formats it according to specific instructions, and then compares the AI-generated answers to the actual answers within a defined tolerance level to determine correctness.\n",
    "\n",
    "Key Features:\n",
    "- Loads training data from a JSON file and iterates through a specified number of questions.\n",
    "- Utilizes Google Gemini and OpenAI API's for generating answers.\n",
    "- Formats the input for the AI model according to detailed instructions, ensuring the model receives all necessary context.\n",
    "- Compares the AI-generated answers to the actual answers, taking into account a tolerance for numerical comparison.\n",
    "- Updates and displays a plot in real-time to visualize the number of correct answers versus the number of questions asked using Plotly.\n",
    "\n",
    "Requirements:\n",
    "- Google Generative AI and Open AI libraries\n",
    "- Set your own API Keys as system variables\n",
    "- Plotly for visualization\n",
    "- JSON for data handling\n",
    "- OS, fpdf  and re (regular expression) modules for file and string operations\n",
    "\n",
    "The script aims to provide a quantitative assessment of the AI model's accuracy in a controlled experimental setup, with potential applications in evaluating AI models for research assistance or data analysis tasks.\n",
    "It will then auto generate a report which analyzes and compares the accuracies of the two models. The results for each model performance are detailed separately from the report for you to verify the reports accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from fpdf import FPDF\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.graph_objs import FigureWidget\n",
    "\n",
    "\n",
    "# LLM API KEY Configuration and Initialization\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "model_gemini = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "model_openai=\"gpt-4o\"\n",
    "\n",
    "# Load training data\n",
    "file_path = os.path.join(os.getcwd(), 'Data/train.json')\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "# Variables to controle quetion number and tolerance for comparison of LLM response with actual answers\n",
    "Questions_to_answer = 81 # Number of pre-text sections (some sections may have multiple questions)\n",
    "tolerance = 0.04 # accounts for rounding differnce in LLM response\n",
    "\n",
    "# Load the prompt for LLM ansering quetions from data set\n",
    "with open(os.path.join(os.getcwd(), 'answer_prompt.txt'), 'r') as file:\n",
    "    prompt = file.read()\n",
    "    \n",
    "# Load the prompt for LLM gerating a report for the results of the LLMs answering questions\n",
    "with open(os.path.join(os.getcwd(), 'report_prompt.txt'), 'r') as file:\n",
    "    report_prompt = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################  Google Gemini Reponse ##########################################################\n",
    "#Key Performance Indicators\n",
    "google_quetion_nunber_tracker= []\n",
    "google_correct_answer_questions = []\n",
    "google_wrong_answer_questions = []\n",
    "question_details = []\n",
    "\n",
    "# Initialize visualization with whole numbers on the axes\n",
    "fig = FigureWidget(make_subplots(rows=1, cols=1))\n",
    "fig.add_trace(go.Scatter(y=[], mode='lines+markers'), row=1, col=1)\n",
    "fig.update_layout(title=\"Number of Correct Answers Vs Questions Asked\",\n",
    "                  xaxis_title=\"Attempt\",\n",
    "                  yaxis_title=\"Number of Correct Answers\")\n",
    "display(fig)  # Use display() for Jupyter Notebooks\n",
    "\n",
    "def update_plot(correct_answers):\n",
    "    \"\"\"Updates the plot with the latest number of correct answers.\"\"\"\n",
    "    google_quetion_nunber_tracker.append(correct_answers)\n",
    "    x_values = list(range(1, len(google_quetion_nunber_tracker) + 1))\n",
    "    fig.data[0].x = x_values\n",
    "    fig.data[0].y = google_quetion_nunber_tracker\n",
    "\n",
    "# Main loop for processing questions with LLM\n",
    "for question_number in range(Questions_to_answer):\n",
    "  \n",
    "    # Extract keys from the data for each question\n",
    "    keys = list(data[question_number].keys())\n",
    "    input_message = \"pre_text = \" + str(data[question_number]['pre_text']) + \\\n",
    "                    \"\\npost_text = \" + str(data[question_number]['post_text']) + \\\n",
    "                    \"\\ntable_ori = \" + str(data[question_number]['table_ori']) + \\\n",
    "                    \"\\ntable = \" + str(data[question_number]['table'])\n",
    "\n",
    "    # Construct input message and collect answers for varying number of quetions for each section\n",
    "    question_answers = \"\"\n",
    "    if 'qa' in keys:\n",
    "        input_message += \"\\nqa = \" + str(data[question_number]['qa']['question'])\n",
    "        question_answers = str(data[question_number]['qa']['answer']) + \", \"\n",
    "    else:\n",
    "        max_index = max([int(key.split('_')[1]) for key in keys if key.startswith('qa')])\n",
    "        for i in range(max_index + 1):\n",
    "            question_key = f'qa_{i}'\n",
    "            if question_key in data[question_number]:\n",
    "                input_message += \"\\n\" + str(question_key) + \" = \" + str(data[question_number][question_key]['question'])\n",
    "                question_answers += str(data[question_number][question_key]['answer']) + \", \"\n",
    "                question_details.append(\"QUESTION NUMBER INFORMATION\" + str(question_number) + \"\\n\" + input_message + \"\\nCorrect Answer\" + str(data[question_number][question_key]['answer']) + \"\\n\")\n",
    "\n",
    "  # Google Gemini query and collect response\n",
    "    assistant_response = str(model_gemini.generate_content([prompt, input_message]).text)\n",
    "\n",
    "    question_answers_list = [x.strip() for x in question_answers.split(',') if x.strip()]\n",
    "    assistant_response_list = [x.strip() for x in assistant_response.split(',') if x.strip()]\n",
    "\n",
    "    # Compare each answer within the tolerance and update results plot\n",
    "    for actual, assistant in zip(question_answers_list, assistant_response_list):\n",
    "        actual_value = float(re.search(r\"-?\\d+(\\.\\d+)?\", actual).group()) if re.search(r\"-?\\d+(\\.\\d+)?\", actual) else None\n",
    "        assistant_value = float(re.search(r\"-?\\d+(\\.\\d+)?\", assistant).group()) if re.search(r\"-?\\d+(\\.\\d+)?\", assistant) else None\n",
    "\n",
    "        # Checks if quetion number has appeard befor and if so adds a suffix to the quetion number \n",
    "        base_question_str = str(question_number + 1)\n",
    "        if actual_value is not None and assistant_value is not None:\n",
    "            if abs(actual_value - assistant_value) / abs(actual_value) <= tolerance:\n",
    "                if base_question_str in google_correct_answer_questions:\n",
    "                    suffix = chr(97 + google_correct_answer_questions.count(base_question_str))  # Start from 'b'\n",
    "                    google_correct_answer_questions.append(f\"{base_question_str}{suffix}\")\n",
    "                else:\n",
    "                    google_correct_answer_questions.append(base_question_str)\n",
    "            else:\n",
    "                if base_question_str in google_wrong_answer_questions:\n",
    "                    suffix = chr(97 + google_wrong_answer_questions.count(base_question_str))  # Start from 'b'\n",
    "                    google_wrong_answer_questions.append(f\"{base_question_str}{suffix}\")\n",
    "                else:\n",
    "                    google_wrong_answer_questions.append(base_question_str)\n",
    "            update_plot(len(google_correct_answer_questions))         \n",
    "\n",
    "# Summarise final results to check LLM report results\n",
    "print(\"Total correct answers: \" + str(len(google_correct_answer_questions)))\n",
    "print(\"Correct answer questions: \" + str(google_correct_answer_questions))\n",
    "print(\"Total wrong answers: \" + str(len(google_wrong_answer_questions)))\n",
    "print(\"Wrong answer questions: \" + str(google_wrong_answer_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################  ApenAI ChatGPT Reponse ##########################################################\n",
    "#Key Performance Indicators\n",
    "openai_quetion_nunber_tracker= []\n",
    "openai_correct_answer_questions = []\n",
    "openai_wrong_answer_questions = []\n",
    "question_details = []\n",
    "\n",
    "# Initialize visualization with whole numbers on the axes\n",
    "fig1 = FigureWidget(make_subplots(rows=1, cols=1))\n",
    "fig1.add_trace(go.Scatter(y=[], mode='lines+markers'), row=1, col=1)\n",
    "fig1.update_layout(title=\"Number of Correct Answers Vs Questions Asked\",\n",
    "                  xaxis_title=\"Attempt\",\n",
    "                  yaxis_title=\"Number of Correct Answers\")\n",
    "display(fig1)  # Use display() for Jupyter Notebooks\n",
    "\n",
    "def update_plot(correct_answers1):\n",
    "    \"\"\"Updates the plot with the latest number of correct answers.\"\"\"\n",
    "    openai_quetion_nunber_tracker.append(correct_answers1)\n",
    "    x_values = list(range(1, len(openai_quetion_nunber_tracker) + 1))\n",
    "    fig1.data[0].x = x_values\n",
    "    fig1.data[0].y = openai_quetion_nunber_tracker\n",
    "\n",
    "# Main loop for processing questions with LLM\n",
    "for question_number in range(Questions_to_answer):\n",
    "  \n",
    "    # Extract keys from the data for each question\n",
    "    keys = list(data[question_number].keys())\n",
    "    input_message = \"pre_text = \" + str(data[question_number]['pre_text']) + \\\n",
    "                    \"\\npost_text = \" + str(data[question_number]['post_text']) + \\\n",
    "                    \"\\ntable_ori = \" + str(data[question_number]['table_ori']) + \\\n",
    "                    \"\\ntable = \" + str(data[question_number]['table'])\n",
    "\n",
    "    # Construct input message and collect answers for varying number of quetions for each section\n",
    "    question_answers = \"\"\n",
    "    if 'qa' in keys:\n",
    "        input_message += \"\\nqa = \" + str(data[question_number]['qa']['question'])\n",
    "        question_answers = str(data[question_number]['qa']['answer']) + \", \"\n",
    "    else:\n",
    "        max_index = max([int(key.split('_')[1]) for key in keys if key.startswith('qa')])\n",
    "        for i in range(max_index + 1):\n",
    "            question_key = f'qa_{i}'\n",
    "            if question_key in data[question_number]:\n",
    "                input_message += \"\\n\" + str(question_key) + \" = \" + str(data[question_number][question_key]['question'])\n",
    "                question_answers += str(data[question_number][question_key]['answer']) + \", \"\n",
    "                question_details.append(\"QUESTION NUMBER INFORMATION\" + str(question_number) + \"\\n\" + input_message + \"\\nCorrect Answer\" + str(data[question_number][question_key]['answer']) + \"\\n\")\n",
    "\n",
    "    # OpenAI query and collect response\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": input_message}]\n",
    "    completion = client.chat.completions.create(model= model_openai, messages=messages)\n",
    "    assistant_response = completion.choices[0].message.content\n",
    "\n",
    "    question_answers_list = [x.strip() for x in question_answers.split(',') if x.strip()]\n",
    "    assistant_response_list = [x.strip() for x in assistant_response.split(',') if x.strip()]\n",
    "\n",
    "    # Compare each answer within the tolerance and update results plot\n",
    "    for actual, assistant in zip(question_answers_list, assistant_response_list):\n",
    "        actual_value = float(re.search(r\"-?\\d+(\\.\\d+)?\", actual).group()) if re.search(r\"-?\\d+(\\.\\d+)?\", actual) else None\n",
    "        assistant_value = float(re.search(r\"-?\\d+(\\.\\d+)?\", assistant).group()) if re.search(r\"-?\\d+(\\.\\d+)?\", assistant) else None\n",
    "\n",
    "        # Checks if quetion number has appeard befor and if so adds a suffix to the quetion number  \n",
    "        base_question_str = str(question_number + 1)\n",
    "        if actual_value is not None and assistant_value is not None:\n",
    "            if abs(actual_value - assistant_value) / abs(actual_value) <= tolerance:\n",
    "                if base_question_str in openai_correct_answer_questions:\n",
    "                    suffix = chr(97 + openai_correct_answer_questions.count(base_question_str))  # Start from 'b'\n",
    "                    openai_correct_answer_questions.append(f\"{base_question_str}{suffix}\")\n",
    "                else:\n",
    "                    openai_correct_answer_questions.append(base_question_str)\n",
    "            else:\n",
    "                if base_question_str in openai_wrong_answer_questions:\n",
    "                    suffix = chr(97 + openai_wrong_answer_questions.count(base_question_str))  # Start from 'b'\n",
    "                    openai_wrong_answer_questions.append(f\"{base_question_str}{suffix}\")\n",
    "                else:\n",
    "                    openai_wrong_answer_questions.append(base_question_str)\n",
    "            update_plot(len(openai_correct_answer_questions))\n",
    "          \n",
    "# Summarise final results to check LLM report results\n",
    "print(\"Total correct answers: \" + str(len(openai_correct_answer_questions)))\n",
    "print(\"Correct answer questions: \" + str(openai_correct_answer_questions))\n",
    "print(\"Total wrong answers: \" + str(len(openai_wrong_answer_questions)))\n",
    "print(\"Wrong answer questions: \" + str(openai_wrong_answer_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Auto Report Generation on LLM Performance\n",
    "\n",
    "This section of the code is dedicated to generating a comprehensive report based on the output results of the Language Model (LLM) in response to the questions. It includes the calculation of system accuracies, a detailed analysis of the findings, and an honest discussion of the system's shortcomings. The report aims to provide insights into the LLM's performance, highlighting areas of success and identifying opportunities for improvement.\n",
    "\n",
    "The report is generated using a LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_input_message = \"****Google Gemini Performance Report****\" + \\\n",
    "               \"\\n Correct answers number: \" + str(len(google_correct_answer_questions)) + \\\n",
    "               \"\\nCorrect answer questions: \" + str(google_correct_answer_questions) + \\\n",
    "               \"\\nWrong answers number : \" + str(len(google_wrong_answer_questions)) + \\\n",
    "               \"\\nWrong answer questions: \" + str(google_wrong_answer_questions) + \\\n",
    "               \"\\n\" + \\\n",
    "               \"****OpenAI Chatgot Performance Report****\" + \\\n",
    "               \"\\n Correct answers number: \" + str(len(openai_correct_answer_questions)) + \\\n",
    "               \"\\nCorrect answer questions: \" + str(openai_correct_answer_questions) + \\\n",
    "               \"\\nWrong answers number : \" + str(len(openai_wrong_answer_questions)) + \\\n",
    "               \"\\nWrong answer questions: \" + str(openai_wrong_answer_questions) +  \\\n",
    "               \"\\n\" + \\\n",
    "               \"\\nQuetion number information: \" + str(question_details)\n",
    "\n",
    "assistant_response = str(model_gemini.generate_content([report_prompt, findings_input_message]).text)\n",
    "\n",
    "print(assistant_response)\n",
    "\n",
    "# Save the LLM report reponse as a pdf file\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "pdf.cell(200, 10, txt=\"LLM Response Report\", ln=True, align='C')\n",
    "pdf.multi_cell(0, 10, assistant_response)\n",
    "pdf_file_path = \"AI_Assignment_Report.pdf\"\n",
    "pdf.output(pdf_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
